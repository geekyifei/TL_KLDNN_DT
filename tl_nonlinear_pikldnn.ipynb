{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Denpendencies\n",
    "import numpy as np\n",
    "import numpy.linalg as npl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import scipy.linalg as spl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Useful Helper functions\n",
    "\n",
    "rl2e = lambda yest, yref : spl.norm(yest - yref, 2) / spl.norm(yref, 2) # Relative L2 error\n",
    "ml2e = lambda yest, yref : 1/yref.shape[0]*spl.norm(yest - yref, 2) # Mean L2 error\n",
    "infe = lambda yest, yref : spl.norm(yest - yref, np.inf) # L-Infinity error\n",
    "\n",
    "\n",
    "def darcy_solver_implicit(K, Nt, Nx, ul, ur, u0, dt, dx):\n",
    "    \n",
    "    u = np.zeros((Nt, Nx))\n",
    "    u[0, :] = u0\n",
    "    uFD = u0*np.ones(Nx)\n",
    "    \n",
    "    A = np.zeros((Nx, Nx))\n",
    "    A[0, 0] = 1\n",
    "    A[-1, -1] = 1\n",
    "    CN = 2 * dt / dx**2\n",
    "    for i in range(1, Nx - 1):\n",
    "        A[i, i] = CN * (K[i + 1] * K[i] / (K[i + 1] + K[i]) + K[i] * K[i - 1] / (K[i] + K[i - 1])) + 1\n",
    "        A[i, i - 1] = -CN * K[i] * K[i - 1] / (K[i] + K[i - 1])\n",
    "        A[i, i + 1] = -CN * K[i + 1] * K[i] / (K[i + 1] + K[i])\n",
    "\n",
    "    for l in range(1, Nt):\n",
    "        b = uFD.copy()\n",
    "        b[0] = ul\n",
    "        b[-1] = ur\n",
    "        uFD = np.linalg.solve(A, b)\n",
    "        u[l, :] = uFD\n",
    "\n",
    "    return u\n",
    "\n",
    "def get_eigenpairs(cov, eps = np.sqrt(np.finfo(float).eps)): #np.sqrt(np.finfo(float).eps)\n",
    "    \n",
    "    # Compute eigendecomposition based on mc emsembles\n",
    "    # return eigenvalues in descending order and eigenvectors are in the column\n",
    "\n",
    "    Lambda, Phi = spl.eigh(cov + eps * np.eye(cov.shape[0]))  #(N, ) (N, N)\n",
    "    return (Phi.real @ np.diag(np.sqrt(np.abs(Lambda))))[:, ::-1], Lambda[::-1]\n",
    "\n",
    "\n",
    "def darcy_residual(meanK, PhiK, xiK, meanU, PhiU, eta,  Nt, Nx, dt, dx):\n",
    "    \n",
    "    K = meanK + PhiK@xiK\n",
    "    u = (meanU + PhiU@eta).reshape((Nt, Nx))\n",
    "    LHS = np.zeros((Nt, Nx))\n",
    "    RHS = np.zeros((Nt, Nx))\n",
    "    residual = np.zeros((Nt, Nx))\n",
    "    \n",
    "    # time index starts from the second index to the last\n",
    "    # space index starts from the second to the second last (cuz there are second-order derivatives)\n",
    "    # If not, there will be very large residuals at the boundary\n",
    "    \n",
    "    LHS[1:, 1:-1] = (u[1:, 1:-1] - u[:-1, 1:-1])/dt \n",
    "    K_eff = 2 * K[1:] * K[:-1] / (K[1:] + K[:-1]) # effective conductivity\n",
    "    RHS[1:,1:-1] = (K_eff[1:] * (u[1:, 2:] - u[1:, 1:-1]) # right flux \n",
    "                   - K_eff[:-1]* (u[1:, 1:-1] - u[1:, :-2])) / dx**2 # left flux\n",
    "    residual = LHS - RHS\n",
    "    \n",
    "    return residual\n",
    "\n",
    "class KLDNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer_nn, \n",
    "                     Nt, Nx, xiK_r, dt, dx,\n",
    "                     meanK, PhiK, meanU, PhiU, \n",
    "                     dtype, device):\n",
    "        \n",
    "        super(KLDNN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(layer_nn) - 1):\n",
    "            self.layers.append(nn.Linear(layer_nn[i], layer_nn[i+1], dtype = dtype, device = device))\n",
    "        \n",
    "        #self.linear1 =  nn.Linear(layer_nn[0], layer_nn[1], dtype = self.dtype, device = self.device)\n",
    "        #self.hook = self.layers[-1].register_forward_hook(self.hook_fn)\n",
    "        \n",
    "        self.Nt = Nt\n",
    "        self.Nx = Nx\n",
    "        self.xiK_r = xiK_r\n",
    "        self.dt = dt\n",
    "        self.dx = dx\n",
    "        self.NKxi = layer_nn[0]\n",
    "        self.Nuxi = layer_nn[-1]\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        \n",
    "        self.meanK = torch.tensor(meanK.copy(), dtype = self.dtype).to(self.device)\n",
    "        self.PhiK = torch.tensor(PhiK.copy(), dtype = self.dtype).to(self.device)\n",
    "        self.PhiU = torch.tensor(PhiU.copy(), dtype = self.dtype).to(self.device)\n",
    "        self.meanU = torch.tensor(meanU.copy(), dtype = self.dtype).to(self.device)\n",
    "        \n",
    "        self.PhiU_rs = self.PhiU.reshape((self.Nt, self.Nx, self.Nuxi))\n",
    "        self.meanU_rs = self.meanU.reshape((self.Nt, self.Nx))\n",
    "        \n",
    "        # the first derivative of u mean w.r.t t\n",
    "        self.d1tdmeanU = torch.zeros((self.Nt, self.Nx), dtype = self.dtype, device = self.device)\n",
    "        self.d1tdmeanU[1:, 1:-1] = (self.meanU_rs[1:, 1:-1] - self.meanU_rs[:-1, 1:-1]) / self.dt\n",
    "        # the first derivative of u eigenfunctions w.r.t t\n",
    "        self.d1tdPhiU = torch.zeros((self.Nt, self.Nx, self.Nuxi), dtype = self.dtype, device = self.device)\n",
    "        self.d1tdPhiU[1:, 1:-1, :] = (self.PhiU_rs[1:, 1:-1, :] - self.PhiU_rs[:-1, 1:-1, :]) / self.dt\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if i < len(self.layers) - 1:  \n",
    "                x = nn.functional.relu(x)\n",
    "        return x\n",
    "    \n",
    "    def d2xdmeanUK(self, xiK_batch):\n",
    "        \n",
    "        Nbatch = xiK_batch.shape[0]\n",
    "        K = torch.einsum('ij,nj->ni', self.PhiK, xiK_batch) + self.meanK  # (Nbatch, Nrx-1) \n",
    "        K_eff = (2 * K[:, :-1] * K[:, 1:] / (K[:, :-1] + K[:, 1:])).reshape(Nbatch, 1, self.Nx - 1)\n",
    "        d2xdmeanUK_ = torch.zeros((Nbatch, self.Nt, self.Nx), dtype = self.dtype).to(self.device)\n",
    "        d2xdmeanUK_[:, 1:, 1:-1] = (K_eff[:, :, 1:] * (self.meanU_rs[None, 1:, 2:] - self.meanU_rs[None, 1:, 1:-1]) -\n",
    "                                    K_eff[:, :, :-1] * (self.meanU_rs[None, 1:, 1:-1] - self.meanU_rs[None, 1:, :-2])) / self.dx**2\n",
    "        \n",
    "        return d2xdmeanUK_\n",
    "    \n",
    "    def d2xdPhiUK(self, xiK_batch):\n",
    "        \n",
    "        Nbatch = xiK_batch.shape[0]\n",
    "        K = torch.einsum('ij,nj->ni', self.PhiK, xiK_batch) + self.meanK  \n",
    "        K_eff = (2 * K[:, :-1] * K[:, 1:] / (K[:, :-1] + K[:, 1:])).reshape(Nbatch, 1, self.Nx - 1, 1)\n",
    "        d2xdPhiUK_ = torch.zeros((Nbatch, Nrt, Nrx, Nuxi), dtype = self.dtype).to(self.device)\n",
    "        d2xdPhiUK_[:, 1:, 1:-1, :] = (K_eff[:, :, 1:, :] * (self.PhiU_rs[None, 1:, 2:, :] - self.PhiU_rs[None, 1:, 1:-1, :]) -\n",
    "                                      K_eff[:, :, :-1, :] * (self.PhiU_rs[None, 1:, 1:-1, :] - self.PhiU_rs[None, 1:, :-2, :])) / self.dx**2\n",
    "        return d2xdPhiUK_\n",
    "    \n",
    "    def residual(self, xiK_batch, xiU_batch):\n",
    "        \n",
    "        A = self.d1tdPhiU - self.d2xdPhiUK(xiK_batch) # (batch, Nt, Nx, Nuxi)\n",
    "        b = self.d1tdmeanU - self.d2xdmeanUK(xiK_batch) # (batch, Nt, Nx)\n",
    "        return torch.einsum('nijk,nk->nij', A, xiU_batch) + b # (batch, Nt, Nx)\n",
    "    \n",
    "    def residual_loss(self, xiK):\n",
    "        \n",
    "        xiU_pred = self.forward(xiK)\n",
    "        residual = self.residual(xiK, xiU_pred)\n",
    "        loss = torch.mean(torch.norm(residual, dim = (1,2)))\n",
    "        return loss\n",
    "        \n",
    "    def train_model(self, input_train, output_train, \n",
    "                    input_test, output_test, lambda_r, optim, batch_size, num_epoch, plot_loss = True):\n",
    "        \n",
    "        # lambda_r for enforce physics constraints\n",
    "        loss_train = []\n",
    "        data_loss_train = []\n",
    "        res_loss_train = []\n",
    "        loss_test = []\n",
    "        train_dataset = TensorDataset(input_train, output_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "        \n",
    "        since = time.time()\n",
    "        if lambda_r == 0:\n",
    "            for epoch in range(num_epoch):\n",
    "                for input_batch, output_batch in train_loader:\n",
    "                    self.train()\n",
    "                    optim.zero_grad()\n",
    "                    output_batch_pred = self.forward(input_batch)\n",
    "                    loss = nn.MSELoss()(output_batch_pred, output_batch)\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "                loss_train.append(loss.detach().cpu().numpy())\n",
    "                \n",
    "                if epoch%num_print == 0:\n",
    "                    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "                \n",
    "                with torch.inference_mode():\n",
    "                    self.eval()\n",
    "                    output_test_pred = self.forward(input_test)\n",
    "                    test_loss = nn.MSELoss()(output_test_pred, output_test)\n",
    "                loss_test.append(test_loss.detach().cpu().numpy())\n",
    "                \n",
    "        else:\n",
    "            for epoch in range(num_epoch):\n",
    "                for input_batch, output_batch in train_loader:\n",
    "                    self.train()\n",
    "                    optim.zero_grad()\n",
    "                    output_batch_pred = self.forward(input_batch)\n",
    "                    data_loss_train_ = nn.MSELoss()(output_batch_pred, output_batch)\n",
    "                    res_loss_train_ = self.residual_loss(xiK_r)\n",
    "                    #loss_train_ = data_loss_train_ + lambda_r*res_loss_train_\n",
    "                    loss_train_ = res_loss_train_\n",
    "                    loss_train_.backward()\n",
    "                    optim.step()\n",
    "                loss_train.append(loss_train_.detach().cpu().numpy())\n",
    "                data_loss_train.append(data_loss_train_.detach().cpu().numpy())\n",
    "                res_loss_train.append((lambda_r*res_loss_train_).detach().cpu().numpy())\n",
    "                \n",
    "                if epoch%num_print == 0:\n",
    "                    print(f'Epoch: {epoch:03d}, Loss:{loss_train_:.3f}, Data:{data_loss_train_:.3f}, Physics:{lambda_r*res_loss_train_:.3f}')\n",
    "                \n",
    "                with torch.inference_mode():\n",
    "                    self.eval()\n",
    "                    output_test_pred = self.forward(input_test)\n",
    "                    data_loss_test = nn.MSELoss()(output_test_pred, output_test)\n",
    "                    res_loss_test = self.residual_loss(xiK_r)\n",
    "                    #loss_test_ = data_loss_test + lambda_r*res_loss_test\n",
    "                    loss_test_ = res_loss_test\n",
    "                loss_test.append(loss_test_.detach().cpu().numpy())\n",
    "            \n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training time: {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        \n",
    "        if plot_loss == True:\n",
    "            plt.figure(figsize=(4,4), dpi = 300)\n",
    "            plt.title(f'epoch={num_epoch},batch={batch_size}')\n",
    "            plt.plot(np.arange(num_epoch), loss_train, label = 'Train loss')\n",
    "            plt.plot(np.arange(num_epoch), loss_test, label = 'Test loss')\n",
    "            if lambda_r != 0:\n",
    "                plt.plot(np.arange(num_epoch), data_loss_train, label = 'Data loss')\n",
    "                plt.plot(np.arange(num_epoch), res_loss_train, label = 'Residual loss')\n",
    "            plt.xlabel('Number of epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend(loc='best')\n",
    "            plt.show()\n",
    "        \n",
    "        return loss_train, loss_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Setup for the non-linear problem\n",
    "For linear PDE problems, the moment equations for $\\overline{h}$, $h'$, and $C_h$ are decoupled. However, the moment equations are coupled for nonlinear PDE problems, which might affect the transferability of the parameters in the KL-NN model. Here, we consider the one-dimensional diffusion equation with the space-dependent control variable $k(x)$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h}{\\partial t} = \\frac{\\partial}{\\partial x} \\left[ k(x) \\frac{\\partial h}{\\partial x} \\right], \\quad x \\in [0, L], \\quad t \\in [0, T],\n",
    "$$\n",
    "\n",
    "subject to the initial condition:\n",
    "\n",
    "$$\n",
    "h(x, t=0) = h_0, \\quad x \\in [0, L],\n",
    "$$\n",
    "\n",
    "and the boundary conditions:\n",
    "\n",
    "$$\n",
    "h(x=0, t) = h_l, \\quad h(x=L, t) = h_r, \\quad t \\in [0, T].\n",
    "$$\n",
    "\n",
    "This equation, among other applications, describes the flow in a confined aquifer with a heterogeneous conductivity field. We model the control variable $k(x)$ with the KLD:\n",
    "\n",
    "$$\n",
    "k(x) \\approx \\mathcal{KL} \\left[\\overline{k}, \\bm\\psi_k, \\bm\\xi_k \\right],\n",
    "$$\n",
    "\n",
    "and the surrogate model takes the form:\n",
    "\n",
    "$$\n",
    "\\hat{h}(x, t | k; \\bm{\\theta}) = \\mathcal{KL} \\left[\\overline{h}, \\bm\\psi_h, \\mathcal{NN}(\\bm\\xi_k; \\bm\\theta) \\right].\n",
    "$$\n",
    "\n",
    "To simplify the analysis, we further assume that the initial condition $h_0$ and boundary conditions $h_l$ and $h_r$ are <span style=\"color:blue\"> deterministic <span> and <span style=\"color:blue\"> different <span> for the source and target problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Model setup\n",
    "# =============================================================================\n",
    "seed = 15\n",
    "rng = np.random.default_rng(seed)\n",
    "L = 1.0 # Domain length\n",
    "Disp = 10\n",
    "\n",
    "# Source Dirichlet BC at x=0 and x=L\n",
    "ul_s = 1 + 0.05 * L\n",
    "ur_s = 1 - 0.05 * L\n",
    "u0_s = ul_s # initial condition\n",
    "\n",
    "# Target Dirichlet BC at x=0 and x=L\n",
    "ul_t = ul_s - 0.1 * L\n",
    "ur_t = ur_s + 0.1 * L\n",
    "u0_t = ul_t # initial condition\n",
    "\n",
    "load_data = True\n",
    "Nmc_s = 1000\n",
    "Nmc_t = 7 #7, 13, 25, 100\n",
    "Nrx = 30  # Number of residual points in space\n",
    "Nrt = 250  # Number of residual points in time\n",
    "dx = L / (Nrx - 1)  # Grid size\n",
    "dt = dx**2 / Disp  # Time step satisfying Courant condition\n",
    "T = dt * (Nrt - 1)  # Time length\n",
    "tr = np.linspace(0, T, Nrt)\n",
    "xr = np.linspace(0, L, Nrx)\n",
    "XX, TT = np.meshgrid(xr, tr)\n",
    "Nr = Nrx * Nrt  # Number of residual points in space-time\n",
    "NYxi = 20  # Number of KL terms in the Y=lnK expansions\n",
    "NKxi = 20  # Number of KL terms in the K expansions\n",
    "Nuxi = 40  # Number of KL terms in the u expansions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reference gaussian Y field and K field\n",
    "gammaY, varY = 0.5, 0.3\n",
    "CovY = varY * np.exp(- (xr[:, np.newaxis] - xr)**2 / gammaY**2)\n",
    "PhiY, LambdaY = get_eigenpairs(CovY)  \n",
    "#cumY = np.cumsum(LambdaY)/np.sum(LambdaY)\n",
    "\n",
    "xiY_s = rng.standard_normal((Nmc_s, NYxi))  \n",
    "Kmc_s = Disp * np.exp(np.einsum('ij,kj->ki', PhiY[:, :NYxi], xiY_s)) # Ensemble of K data\n",
    "meanK = np.mean(Kmc_s, axis = 0)\n",
    "covK = np.cov(Kmc_s.T) \n",
    "PhiK, LambdaK = get_eigenpairs(covK)\n",
    "\n",
    "# \"Effective\" K   \n",
    "xiY_ref = rng.standard_normal(NYxi) \n",
    "K_ref = Disp * np.exp(PhiY[:, :NYxi] @ xiY_ref)\n",
    "K_eff = np.zeros(Nrx - 1)\n",
    "for i in range(Nrx - 1): # Effective conductivity\n",
    "    K_eff[i] = 2 * K_ref[i] * K_ref[i + 1] / (K_ref[i] + K_ref[i + 1])\n",
    "xiK_ref = np.linalg.lstsq(PhiK[:,:NKxi], (K_ref - meanK).T, rcond=-1)[0].T # LS fitting\n",
    "\n",
    "meanK_eff = np.zeros(Nrx - 1)  \n",
    "PhiK_eff = np.zeros((Nrx - 1, NKxi))\n",
    "for i in range(Nrx - 1):\n",
    "    meanK_eff[i] = 2 * meanK[i] * meanK[i + 1] / (meanK[i] + meanK[i + 1])\n",
    "    for j in range(NKxi):\n",
    "        PhiK_eff[i, j] = 2 * PhiK[i, j] * PhiK[i + 1, j] / (PhiK[i, j] + PhiK[i + 1, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_data == True:\n",
    "    with h5py.File(f'./tl_kldnn_multiplicative_varY_{varY}_seed_{seed}.h5', 'r') as f:\n",
    "        #print([key for key in f.keys()])    \n",
    "        umc_s = f.get('u_source')[:] \n",
    "else:\n",
    "    umc_s = np.zeros((Nmc_s, Nrt * Nrx))  # Ensemble of u data\n",
    "    for i in range(Nmc_s):\n",
    "        usol = darcy_solver_implicit(Kmc_s[i, :], Nrt, Nrx, ul_s, ur_s, u0_s, dt, dx)\n",
    "        umc_s[i, :] = usol.reshape(-1)\n",
    "    with h5py.File(f'./tl_kldnn_multiplicative_varY_{varY}_seed_{seed}.h5', 'w') as f:\n",
    "        f.create_dataset('u_source', data = umc_s)\n",
    "        \n",
    "meanU_s = np.mean(umc_s, axis = 0)\n",
    "covU = np.cov(umc_s.T) \n",
    "PhiU, LambdaU = get_eigenpairs(covU)\n",
    "PhiU_rs = PhiU[:,:Nuxi].reshape((Nrt, Nrx, Nuxi))\n",
    "meanU_rs = meanU_s.reshape((Nrt, Nrx))\n",
    "\n",
    "rng2 = np.random.default_rng(6)\n",
    "umc_t = np.zeros((Nmc_t, Nrt*Nrx))\n",
    "Kmc_t = np.zeros((Nmc_t, Nrx))  \n",
    "for i in range(Nmc_t):\n",
    "    xiY_t = rng2.standard_normal(NYxi)  \n",
    "    K_t = Disp * np.exp(np.einsum('ij,j->i', PhiY[:, :NYxi], xiY_t))\n",
    "    u_t = darcy_solver_implicit(K_t, Nrt, Nrx, ul_t, ur_t, u0_t, dt, dx)\n",
    "    Kmc_t[i, :] = K_t\n",
    "    umc_t[i, :] = u_t.ravel()\n",
    "\n",
    "# The reference data\n",
    "u_ref_s = darcy_solver_implicit(K_ref, Nrt, Nrx, ul_s, ur_s, u0_s, dt, dx)\n",
    "u_ref_t = darcy_solver_implicit(K_ref, Nrt, Nrx, ul_t, ur_t, u0_t, dt, dx)\n",
    "meanU_t = darcy_solver_implicit(meanK, Nrt, Nrx, ul_t, ur_t, u0_t, dt, dx).reshape(-1)\n",
    "xiU_ref_s = np.linalg.lstsq(PhiU[:,:Nuxi], (u_ref_s.ravel() - meanU_s).T, rcond=-1)[0].T # LS fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# KL-DNN model\n",
    "# =============================================================================\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float32\n",
    "torch_seed = 42\n",
    "torch.manual_seed(torch_seed)\n",
    "input_dim = NKxi\n",
    "output_dim = Nuxi\n",
    "hidden_size = 50\n",
    "layers_nn = [input_dim, hidden_size, hidden_size, output_dim]\n",
    "batch_size = 50\n",
    "lr = 1e-3\n",
    "num_print = 200\n",
    "num_epoch_s = 4000\n",
    "num_epoch_t = 4000\n",
    "lambda_r_s = 0 # no physcis constraint for source training\n",
    "lambda_r_t = 1e-4\n",
    "Nres = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Data Preparation =================================\n",
    "\n",
    "xiK_r = torch.normal(0, 1, size=(Nres, NKxi), dtype = dtype).to(device)\n",
    "\n",
    "Ns_train = int(Nmc_s*0.8)\n",
    "Ns_test = Nmc_s - Ns_train\n",
    "\n",
    "xiU_s = np.linalg.lstsq(PhiU[:,:Nuxi], (umc_s - meanU_s).T, rcond=-1)[0].T # (Nmc_s, Nuxi)\n",
    "xiK_s = np.linalg.lstsq(PhiK[:,:NKxi], (Kmc_s - meanK).T, rcond=-1)[0].T # (Nmc_s, NKxi)\n",
    "xiU_s_train = torch.tensor(xiU_s[:Ns_train, :], dtype=dtype).to(device)\n",
    "xiK_s_train = torch.tensor(xiK_s[:Ns_train, :], dtype=dtype).to(device)\n",
    "xiU_s_test = torch.tensor(xiU_s[Ns_train:, :], dtype=dtype).to(device)\n",
    "xiK_s_test = torch.tensor(xiK_s[Ns_train:, :], dtype=dtype).to(device)\n",
    "xiK_ref_torch = torch.tensor(xiK_ref[np.newaxis, :], dtype=dtype).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Source model training =================================\n",
    "\n",
    "model_s = KLDNN(layers_nn, \n",
    "              Nrt, Nrx, xiK_r, dt, dx,\n",
    "              meanK, PhiK[:, :NKxi], meanU_s, PhiU[:, :Nuxi], \n",
    "              dtype, device).to(device)\n",
    "for name, param in model_s.named_parameters(): \n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Requires Grad: {param.requires_grad}\")\n",
    "optim_s = torch.optim.Adam(model_s.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim_s, step_size= 100, gamma= 0.5)\n",
    "loss_train, loss_test = model_s.train_model(xiK_s_train, xiU_s_train, \n",
    "                                            xiK_s_test, xiU_s_test, \n",
    "                                            lambda_r_s,\n",
    "                                            optim_s, batch_size, num_epoch_s)\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    xiU_ref_pred = model_s(xiK_ref_torch).cpu().numpy()\n",
    "u_kldnn_s = (meanU_s + PhiU[:, :Nuxi] @ xiU_ref_pred[0, :]).reshape((Nrt, Nrx))\n",
    "print('KL-DNN method for the source condition')\n",
    "print('RL2 error between u pred and u ref for source: {:.3e}'.format(rl2e(u_kldnn_s, u_ref_s)))\n",
    "utils.plot_sol(xr, u_ref_s, u_kldnn_s, Nrt, label = 'source', title = f'$\\sigma^2_y = {varY}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensemble DNN training\n",
    "Nrandom = 20\n",
    "Nt_train = int(Nmc_t*0.8)\n",
    "Nt_test = Nmc_t - Nt_train\n",
    "umc_t = np.zeros((Nmc_t, Nrt*Nrx))\n",
    "Kmc_t = np.zeros((Nmc_t, Nrx))  \n",
    "num_epoch = 4000\n",
    "lr = 1e-3\n",
    "num_print = 200\n",
    "rl2e_arr = []\n",
    "rng_arr = np.concatenate([np.array([42]), np.arange(0, Nrandom)])\n",
    "ukl_mat = np.zeros((Nrandom + 1, Nrt, Nrx))\n",
    "\n",
    "for index, seed in enumerate(rng_arr):\n",
    "    rng2 = np.random.default_rng(seed)\n",
    "    for i in range(Nmc_t):\n",
    "        xiY_t = rng2.standard_normal(NYxi)  \n",
    "        K_t = Disp * np.exp(np.einsum('ij,j->i', PhiY[:, :NYxi], xiY_t))\n",
    "        u_t = darcy_solver_implicit(K_t, Nrt, Nrx, ul_t, ur_t, u0_t, dt, dx)\n",
    "        Kmc_t[i, :] = K_t\n",
    "        umc_t[i, :] = u_t.ravel()\n",
    "    \n",
    "    xiU_t = np.linalg.lstsq(PhiU[:,:Nuxi], (umc_t - meanU_t.ravel()).T, rcond=-1)[0].T #(Nmc_t, Nuxi)\n",
    "    xiK_t = np.linalg.lstsq(PhiK[:,:NKxi], (Kmc_t - meanK).T, rcond=-1)[0].T #(Nmc_t, NKxi)\n",
    "    xiU_t_train = torch.tensor(xiU_t[:Nt_train, :], dtype=dtype).to(device)\n",
    "    xiK_t_train = torch.tensor(xiK_t[:Nt_train, :], dtype=dtype).to(device)\n",
    "    xiU_t_test = torch.tensor(xiU_t[Nt_train:, :], dtype=dtype).to(device)\n",
    "    xiK_t_test = torch.tensor(xiK_t[Nt_train:, :], dtype=dtype).to(device)\n",
    "\n",
    "    model_t = KLDNN(layers_nn, \n",
    "                  Nrt, Nrx, xiK_r, dt, dx,\n",
    "                  meanK, PhiK[:, :NKxi], meanU_t, PhiU[:, :Nuxi], \n",
    "                  dtype, device).to(device)\n",
    "    model_t.load_state_dict(model_s.state_dict(), strict=False)\n",
    "    \n",
    "    for param in model_t.parameters():\n",
    "        param.requires_grad = False  \n",
    "    for param in model_t.layers[-1].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    optim_t = torch.optim.Adam(model_t.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler_t = torch.optim.lr_scheduler.StepLR(optim_t, step_size= 100, gamma= 0.5)\n",
    "    loss_train, loss_test = model_t.train_model(xiK_t_train, xiU_t_train, \n",
    "                                                xiK_t_test, xiU_t_test, \n",
    "                                                lambda_r_t,\n",
    "                                                optim_t, batch_size, num_epoch_t)  \n",
    "    with torch.no_grad():\n",
    "        xiU_ref_pred = model_t(xiK_ref_torch).cpu().numpy()\n",
    "    u_kldnn_t = (meanU_t + PhiU[:, :Nuxi] @ xiU_ref_pred[0, :]).reshape((Nrt, Nrx))\n",
    "\n",
    "    ukl_mat[index, ...] = u_kldnn_t\n",
    "    rl2e_arr.append(rl2e(u_kldnn_t, u_ref_t))\n",
    "print('RL2 error between u pred and u ref for source: {:.3e}'.format(np.mean(rl2e_arr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = np.argmin(rl2e_arr)\n",
    "plt.figure(figsize=(5, 4), dpi=300)\n",
    "plt.plot(xr, u_ref_t[Nrt//50, :], color='#FF5575', linewidth=2.5, label='Ref, $t = t_1$') \n",
    "plt.plot(xr, ukl_mat[index, Nrt//50, :], 'o', color='#FF5575', markersize= 6,  markeredgewidth=2, mfc='none', label='Pred, $t = t_1$')\n",
    "plt.plot(xr, u_ref_t[Nrt//5, :], color='#FFD36A', linewidth=2.5, label='Ref, $t = t_2$')\n",
    "plt.plot(xr, ukl_mat[index, Nrt//5, :], 'o', color='#FFD36A', markersize=  6,  markeredgewidth=2, mfc='none', label='Pred, $t = t_2$')\n",
    "plt.plot(xr, u_ref_t[Nrt - 1, :], color='#6299FF', linewidth=2.5, label='Ref, $t = t_3$')  \n",
    "plt.plot(xr, ukl_mat[index, Nrt - 1, :], 'o', color='#6299FF', markersize= 6,  markeredgewidth=2, mfc='none', label='Pred, $t = t_3$')\n",
    "plt.xlabel(r'$X$', fontsize= 14)\n",
    "plt.ylabel(r'$h$, target', fontsize= 14)\n",
    "plt.xlim(-0.01, 1.01)\n",
    "plt.xticks(fontsize= 14)\n",
    "plt.yticks(fontsize= 14)\n",
    "plt.title(f'$\\sigma^2_y = {varY}$', fontsize = 14)\n",
    "plt.legend(fontsize= 11)  \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "piml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
